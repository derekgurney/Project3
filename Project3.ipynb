{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library imports and file definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using complete file\n",
      "/Users/DerekAtWork/Google Drive/Data Science/Udacity/Project3.Data Wrangling with MongoDB/vancouver_canada.osm\n",
      "/Users/DerekAtWork/Google Drive/Data Science/Udacity/Project3.Data Wrangling with MongoDB/vancouver_canada.osm.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import codecs\n",
    "import json\n",
    "\n",
    "directory = '/Users/DerekAtWork/Google Drive/Data Science/Udacity/Project3.Data Wrangling with MongoDB/'\n",
    "#directory = '/Users/DerekAtWork/GitHub/Udacity/Project3'\n",
    "\n",
    "#sample = True\n",
    "sample = False\n",
    "\n",
    "if sample:\n",
    "    print \"Using sample\"\n",
    "    osmfile = directory + 'sample.osm'\n",
    "    jsonfile = osmfile + \".json\"  \n",
    "else:\n",
    "    print \"Using complete file\"\n",
    "    osmfile = directory + 'vancouver_canada.osm'\n",
    "    jsonfile = osmfile + \".json\"\n",
    "print osmfile\n",
    "print jsonfile\n",
    "filename = osmfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for creating sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import xml.etree.ElementTree as ET  # Use cElementTree or lxml if too slow\n",
    "'''\n",
    "\n",
    "OSM_FILE = osmfile  # Replace this with your osm file\n",
    "SAMPLE_FILE = directory + \"sample.osm\"\n",
    "\n",
    "k = 20 # Parameter: take every k-th top level element\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\n",
    "\n",
    "    Reference:\n",
    "    http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\n",
    "    \"\"\"\n",
    "    context = iter(ET.iterparse(osm_file, events=('start', 'end')))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "with open(SAMPLE_FILE, 'wb') as output:\n",
    "    output.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "    output.write('<osm>\\n  ')\n",
    "\n",
    "    # Write every kth top level element\n",
    "    for i, element in enumerate(get_element(OSM_FILE)):\n",
    "        if i % k == 0:\n",
    "            output.write(ET.tostring(element, encoding='utf-8'))\n",
    "\n",
    "    output.write('</osm>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Types\n",
    "[Same as Lesson 6 code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /Users/DerekAtWork/Google Drive/Data Science/Udacity/Project3.Data Wrangling with MongoDB/vancouver_canada.osm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lower': 235553, 'lower_colon': 25803, 'other': 2851, 'problemchars': 2}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Your task is to explore the data a bit more.\n",
    "Before you process the data and add it into MongoDB, you should check the \"k\"\n",
    "value for each \"<tag>\" and see if they can be valid keys in MongoDB, as well as\n",
    "see if there are any other potential problems.\n",
    "\n",
    "We have provided you with 3 regular expressions to check for certain patterns\n",
    "in the tags. As we saw in the quiz earlier, we would like to change the data\n",
    "model and expand the \"addr:street\" type of keys to a dictionary like this:\n",
    "{\"address\": {\"street\": \"Some value\"}}\n",
    "So, we have to see if we have such tags, and if we have any tags with\n",
    "problematic characters.\n",
    "\n",
    "Please complete the function 'key_type', such that we have a count of each of\n",
    "four tag categories in a dictionary:\n",
    "  \"lower\", for tags that contain only lowercase letters and are valid,\n",
    "  \"lower_colon\", for otherwise valid tags with a colon in their names,\n",
    "  \"problemchars\", for tags with problematic characters, and\n",
    "  \"other\", for other tags that do not fall into the other three categories.\n",
    "See the 'process_map' and 'test' functions for examples of the expected format.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        # YOUR CODE HERE\n",
    "        key = element.get(\"k\") \n",
    "        #print key\n",
    "        if re.match(lower, key): #Note to self: match starting from beginning of string\n",
    "            keys[\"lower\"] += 1\n",
    "            #keys[\"lower\"].append(key)\n",
    "            #print key\n",
    "        elif re.match(lower_colon, key): \n",
    "            keys[\"lower_colon\"]  += 1\n",
    "            #keys[\"lower_colon\"].append(key)\n",
    "            #print key\n",
    "        elif  re.search(problemchars, key): #search anywhere\n",
    "            keys[\"problemchars\"]  += 1\n",
    "            #keys[\"problemchars\"].append(key)\n",
    "             #print key\n",
    "        else:\n",
    "            keys[\"other\"]  += 1\n",
    "\n",
    "                #keys[\"other\"].append(key) \n",
    "                #print key       \n",
    "    return keys\n",
    "\n",
    "\n",
    "\n",
    "def process_map(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "\n",
    "    return keys\n",
    "\n",
    "print \"Using \" + osmfile\n",
    "process_map(osmfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Users\n",
    "[Same as Lesson 6 code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([None,\n",
      "     '2bre99',\n",
      "     '4b696d',\n",
      "     '7503146',\n",
      "     '>LAND',\n",
      "     'ACervantes',\n",
      "     'AKC',\n",
      "     'Aaron Hilton',\n",
      "     'Achilnos',\n",
      "     'Adam Dunn',\n",
      "     'AdamWill',\n",
      "     'Alan',\n",
      "     'Alan Trick',\n",
      "     'Aleks-Berlin',\n",
      "     'Alex Cruise',\n",
      "     'AmateurCartographer',\n",
      "     'AndiG88',\n",
      "     'Andre68',\n",
      "     'Andrea428',\n",
      "     'AndyGibb',\n",
      "     'Anthony Steele',\n",
      "     'Anton Lavrov',\n",
      "     'Aquin',\n",
      "     'Art Olin',\n",
      "     'Arthur Chan',\n",
      "     'BC Hiker',\n",
      "     'BCBiker',\n",
      "     'Bellbellbell',\n",
      "     'Blaireo',\n",
      "     'BlaneG',\n",
      "     'Blobo123',\n",
      "     'Bootprint',\n",
      "     'Brandon5228',\n",
      "     'BruceMap',\n",
      "     'Bryce C Nesbitt',\n",
      "     'BuganiniQ',\n",
      "     'Bullroarer',\n",
      "     'Burrito!',\n",
      "     'C2H0',\n",
      "     'CBDVN',\n",
      "     'CJ60',\n",
      "     'Cababunga',\n",
      "     'CamD',\n",
      "     'CamJW',\n",
      "     'CamNZ',\n",
      "     'CandyFu',\n",
      "     'ChrissW-R1',\n",
      "     'Cicerone',\n",
      "     'Claudius Henrichs',\n",
      "     'CloCkWeRX',\n",
      "     'Cobra',\n",
      "     'Concession',\n",
      "     'CoreyBurger',\n",
      "     'Creando',\n",
      "     'DENelson83',\n",
      "     'DKazzed',\n",
      "     'DMDK',\n",
      "     'DPTRob',\n",
      "     'Dami_Tn',\n",
      "     'DangNgu',\n",
      "     'Danielos94',\n",
      "     'David & Christine Schmitt',\n",
      "     'David Huang',\n",
      "     'David Metcalfe',\n",
      "     'DeniskMikhailov',\n",
      "     'DevManning',\n",
      "     'DoesTheMoonAlsoFall',\n",
      "     'Drdul',\n",
      "     'DuaneElverum',\n",
      "     'Elmo Yu',\n",
      "     'Emarsee',\n",
      "     'Enene',\n",
      "     'Evantime',\n",
      "     'Evelyn Chen',\n",
      "     'Federico Mena Quintero',\n",
      "     'Floydm',\n",
      "     'Francois Marier',\n",
      "     'FvGordon',\n",
      "     'GJSchofield',\n",
      "     'GVRD',\n",
      "     'Geogast',\n",
      "     'GerdP',\n",
      "     'Glassman',\n",
      "     'GoogleMapMaker',\n",
      "     'Green Lake Studio',\n",
      "     'GreenGeographer',\n",
      "     'Hai-Etlik',\n",
      "     'Hawk777',\n",
      "     'HeyFaulk',\n",
      "     'HolgerJeromin',\n",
      "     'Homlett',\n",
      "     'Homsar123',\n",
      "     'Idcmp',\n",
      "     'Iron Beaver',\n",
      "     'JGairns',\n",
      "     'JMolnar',\n",
      "     'Jallensauce',\n",
      "     'JamesB93',\n",
      "     'JamesCopeland',\n",
      "     'JeffJSmith',\n",
      "     'JeffTaylor',\n",
      "     'Jens von Bergmann',\n",
      "     'Jepau',\n",
      "     'JesseVader08',\n",
      "     \"Jim O'Leary\",\n",
      "     'Jochen Topf',\n",
      "     'JoeDuffy90',\n",
      "     'Joel Carter',\n",
      "     'John Kroeker',\n",
      "     'JohnnyDooley',\n",
      "     'Jon Strait',\n",
      "     'Joni1101',\n",
      "     'Josef73',\n",
      "     'Jou64',\n",
      "     'Jubilo Haku',\n",
      "     'KaabiStar',\n",
      "     'KamiCrit',\n",
      "     'Kamil Kisiel',\n",
      "     'KaranM',\n",
      "     'Koinonen',\n",
      "     'L0ken',\n",
      "     'LS_Gossau',\n",
      "     'Larifari77',\n",
      "     'Latze',\n",
      "     'Lawn Gnome',\n",
      "     'Liam Whalen',\n",
      "     'Lindymap',\n",
      "     'Linnea Harder',\n",
      "     'Little Brother',\n",
      "     'LivingWithDragons',\n",
      "     'LucasMedaffy',\n",
      "     'Luke Vivier',\n",
      "     'MB MIGRATION',\n",
      "     'MBartsch',\n",
      "     'MNoo',\n",
      "     'Maggie Yu',\n",
      "     'Manu1400',\n",
      "     'MappityDoo',\n",
      "     'Marcott',\n",
      "     'Markus59',\n",
      "     'Martin Feucht',\n",
      "     'MaryBennett',\n",
      "     'Math1985',\n",
      "     'Mathiasdm',\n",
      "     'Medevac71',\n",
      "     'Meliora Cogito',\n",
      "     'MelioraCogito',\n",
      "     'MetVanRider123acme',\n",
      "     'Michi',\n",
      "     'MikaelN',\n",
      "     'Mike Moody',\n",
      "     'Mike@BC',\n",
      "     'Mo Quinn',\n",
      "     'MonicaW',\n",
      "     'Murphy',\n",
      "     'MuzikMachine',\n",
      "     'NE2',\n",
      "     'Natfoot',\n",
      "     'NathanCH',\n",
      "     'Nautic',\n",
      "     'Neil M',\n",
      "     'Nicholas Fong',\n",
      "     'Nicolas Untz',\n",
      "     'Nihat',\n",
      "     'NoVa mapper',\n",
      "     'North Van Man',\n",
      "     'NuttyNutter',\n",
      "     'OSMF Redaction Account',\n",
      "     'Omnific',\n",
      "     'Oplopanax',\n",
      "     'PFriedrichS',\n",
      "     'PanoWorks',\n",
      "     'Paradox352',\n",
      "     'Paul Johnson',\n",
      "     'Paul Ramsey',\n",
      "     'Pepper',\n",
      "     'Peter Bzzz',\n",
      "     'Peter Eller',\n",
      "     'Peter14',\n",
      "     'PeterEastern',\n",
      "     'PhilB61',\n",
      "     'PlaneMad',\n",
      "     'Porfiry',\n",
      "     'Pronab Saha',\n",
      "     'QsoftStudios',\n",
      "     'Qualifirst Foods',\n",
      "     'R0bst3r',\n",
      "     'RAD',\n",
      "     'RBC123',\n",
      "     'RM87',\n",
      "     'RMorewood',\n",
      "     'RenSylvain',\n",
      "     'RetiredInNH',\n",
      "     'Rhondle',\n",
      "     'RichRico',\n",
      "     'RoadGeek_MD99',\n",
      "     'Roadrunner21',\n",
      "     'Rob Oost',\n",
      "     'RodrigoWarrior',\n",
      "     'Rps333',\n",
      "     'Rupert Swarbrick',\n",
      "     'Ryrainey',\n",
      "     'SP!KE',\n",
      "     'SRW',\n",
      "     'Saarang',\n",
      "     'SabineSW',\n",
      "     'Sam Dal Monte',\n",
      "     'ScottNelson',\n",
      "     'Serpens',\n",
      "     'Shannoncox',\n",
      "     'Shrinks99',\n",
      "     'Shuyan',\n",
      "     'Siegbaert',\n",
      "     'Signco',\n",
      "     'Simoneyes',\n",
      "     'SnowMeeper',\n",
      "     'SomeoneElse_Revert',\n",
      "     'Spacecookies',\n",
      "     'Spectrokid',\n",
      "     'Sphaerophoria',\n",
      "     'Stalfur',\n",
      "     'Stemby',\n",
      "     'Stephen Fung',\n",
      "     'Sundance',\n",
      "     'Sungsu',\n",
      "     'Sunhealth Clinic',\n",
      "     'Sven L',\n",
      "     'Syl',\n",
      "     'Tammy Jin PREC',\n",
      "     'Tantilus',\n",
      "     'TerraKendama',\n",
      "     'TheDutchMan13',\n",
      "     'The_Tom',\n",
      "     'Thibault ML',\n",
      "     'Thorseth',\n",
      "     'Todd Chisholm',\n",
      "     'TomRox',\n",
      "     'Tooben',\n",
      "     'Tovok7',\n",
      "     'TristanA',\n",
      "     'Tsonoqua',\n",
      "     'Tunnen',\n",
      "     'UBC TEST',\n",
      "     'UnChosen',\n",
      "     'VPOman',\n",
      "     'VanAdam',\n",
      "     'Vutu',\n",
      "     'WBSKI',\n",
      "     u'Walter Schl\\xf6gl',\n",
      "     'Wassup789',\n",
      "     'Wenhao Chen',\n",
      "     'WhiteMoose',\n",
      "     'Winston687',\n",
      "     'WonderSteve',\n",
      "     'World_Mapper',\n",
      "     'Xiaobo Wang',\n",
      "     'YVRBee',\n",
      "     'YogaNerd',\n",
      "     'YuruW',\n",
      "     'ZMWandelaar',\n",
      "     'Zandlopertje',\n",
      "     'ZzD7',\n",
      "     'a4audi4fun',\n",
      "     'a_fischer',\n",
      "     'abeckmann',\n",
      "     'abel801',\n",
      "     'adaviel',\n",
      "     'adjuva',\n",
      "     'ahalla',\n",
      "     'aharvey',\n",
      "     'ak1992',\n",
      "     'alarobric',\n",
      "     'alester',\n",
      "     'alester_imports',\n",
      "     'alexwarrior',\n",
      "     'alexz3',\n",
      "     'andrewboktor',\n",
      "     'andrewpmk',\n",
      "     'andygol',\n",
      "     'arcvancouver',\n",
      "     'arekushi',\n",
      "     'arteku',\n",
      "     'bbyrd',\n",
      "     'bennysdad',\n",
      "     'bibliothecar',\n",
      "     'billm2403',\n",
      "     'binhex',\n",
      "     'bitscoast',\n",
      "     'blaueva52h7jk',\n",
      "     'brchookolingo',\n",
      "     'brettcannon',\n",
      "     'bri g',\n",
      "     'brianh',\n",
      "     'bryceco',\n",
      "     'calfarome',\n",
      "     'casye',\n",
      "     'cdavila',\n",
      "     'cgaspoz',\n",
      "     'cgu66',\n",
      "     'charlescm',\n",
      "     'ciclista',\n",
      "     'clairehhlin',\n",
      "     'compdude',\n",
      "     'crisspu',\n",
      "     'crompton',\n",
      "     'csgisr',\n",
      "     'd3r3kk',\n",
      "     'dale_p',\n",
      "     'dannykath',\n",
      "     'darkoverlord',\n",
      "     'david105',\n",
      "     'dbaron',\n",
      "     'dcp',\n",
      "     'debbers',\n",
      "     'developingcountries',\n",
      "     'dhfinch',\n",
      "     'digicult',\n",
      "     'digitaldirect',\n",
      "     'dkmikea',\n",
      "     'dlewis',\n",
      "     'dmgroom',\n",
      "     'doviende',\n",
      "     'dpogue',\n",
      "     'dshkol',\n",
      "     'dtcaciuc',\n",
      "     'dtwong',\n",
      "     'dunbar loop',\n",
      "     'dustindauncey',\n",
      "     'dydychan',\n",
      "     'earlofspencer',\n",
      "     'ecnepsnai',\n",
      "     'ediyes',\n",
      "     'eehmke',\n",
      "     'elbatrop',\n",
      "     'emac',\n",
      "     'emptybits',\n",
      "     'eone',\n",
      "     'etp4eva',\n",
      "     'expandfurniture',\n",
      "     'fanthore',\n",
      "     'felipeedwards',\n",
      "     'findfreddy',\n",
      "     'finn1911',\n",
      "     'fissionchips',\n",
      "     'forrert',\n",
      "     'freemadc',\n",
      "     'freietonne-db',\n",
      "     'funvill',\n",
      "     'fx99',\n",
      "     'gary-prime',\n",
      "     'garynuss',\n",
      "     'gcamp',\n",
      "     'ge0freak',\n",
      "     'geodreieck4711',\n",
      "     'geoffengland',\n",
      "     'geoffsin',\n",
      "     'geozeisig',\n",
      "     'germin8',\n",
      "     'gfish',\n",
      "     'giggls',\n",
      "     'gindo89',\n",
      "     'gkornelson',\n",
      "     'gleissmap',\n",
      "     'gloria_cyb',\n",
      "     'gowlin',\n",
      "     'green-grover',\n",
      "     'gregus_',\n",
      "     'guegafue',\n",
      "     'hannaha',\n",
      "     'hanoj',\n",
      "     'hfourstu',\n",
      "     'hjonescox',\n",
      "     'hmpfgnrrr',\n",
      "     'hofoen',\n",
      "     'homebug',\n",
      "     'homerhsing',\n",
      "     'hpka',\n",
      "     'humbach',\n",
      "     'hyandrew',\n",
      "     'iLoveCrows',\n",
      "     'iainfromthelibrary',\n",
      "     'ij_',\n",
      "     'illoLab',\n",
      "     'imaginet',\n",
      "     'imnskp',\n",
      "     'iplat',\n",
      "     'irmatt',\n",
      "     'j-beda',\n",
      "     'jacobbraeutigam',\n",
      "     'jansenq',\n",
      "     'jaskoh',\n",
      "     'javafern',\n",
      "     'jaypeecee',\n",
      "     'jef',\n",
      "     'jharvey',\n",
      "     'jocehl',\n",
      "     'johncharlesdaly',\n",
      "     'johnnydangermond',\n",
      "     'jorge_o',\n",
      "     'jot',\n",
      "     'jpadams',\n",
      "     'jsattie',\n",
      "     'julianladisch',\n",
      "     'justinsulpico',\n",
      "     'jya86',\n",
      "     'k_re',\n",
      "     'kaitoe138',\n",
      "     'kakrueger',\n",
      "     'kalupa',\n",
      "     'kangaruu',\n",
      "     'karitotp',\n",
      "     'kartler175',\n",
      "     'kdickie',\n",
      "     'keithonearth',\n",
      "     'kelseydh',\n",
      "     'kelsiding',\n",
      "     'kelvinc',\n",
      "     'kerrygis',\n",
      "     'kevinglobal',\n",
      "     'kevlangton',\n",
      "     'kisaa',\n",
      "     'klusark',\n",
      "     'kmkrebs',\n",
      "     'kpelly',\n",
      "     'kuwako315',\n",
      "     'kyoshino',\n",
      "     'lachliggity',\n",
      "     'lasmasi',\n",
      "     'laurentr',\n",
      "     'lbuchy',\n",
      "     'leeching928',\n",
      "     'leeshoal',\n",
      "     'leuty',\n",
      "     'liamvancouver',\n",
      "     'liyuan85',\n",
      "     'ljjwfr',\n",
      "     'lnocheesse',\n",
      "     'lokejul',\n",
      "     'ltreed',\n",
      "     'lunafiko',\n",
      "     'macdonsp',\n",
      "     'mackb94',\n",
      "     'madison555',\n",
      "     'maggot27',\n",
      "     'maitrimover',\n",
      "     'malcolmh',\n",
      "     'mame-stgt',\n",
      "     'mapone',\n",
      "     'mappatman',\n",
      "     'marek kleciak',\n",
      "     'marionv',\n",
      "     'maryjs',\n",
      "     'mash84121',\n",
      "     'mashimaro',\n",
      "     'matteditmsts',\n",
      "     'mattropolis',\n",
      "     'mbiker',\n",
      "     'mbiker_imports_and_more',\n",
      "     'mcastagne',\n",
      "     'mcld',\n",
      "     'mdiener',\n",
      "     'mfagan',\n",
      "     'mhoulne',\n",
      "     'michael_moovelmaps',\n",
      "     'mk801',\n",
      "     'morray',\n",
      "     'mrehayden',\n",
      "     'muratc3',\n",
      "     'mwbrooks',\n",
      "     'myxor',\n",
      "     'normanm',\n",
      "     'nuckster',\n",
      "     'nuxper',\n",
      "     'oceanearth',\n",
      "     'ocgrace',\n",
      "     'ohayden',\n",
      "     'okrauth',\n",
      "     'oldenburg69',\n",
      "     'oldtopos',\n",
      "     'osm-pascal',\n",
      "     'oxy10',\n",
      "     'pacificIT',\n",
      "     'paulschw',\n",
      "     'pbryan',\n",
      "     'pcolli',\n",
      "     'pdunn',\n",
      "     'permute',\n",
      "     'peted',\n",
      "     'peterjohnson',\n",
      "     'petersfreeman',\n",
      "     'phoff',\n",
      "     'pnorman',\n",
      "     'pnorman_mechanical',\n",
      "     'pray4mojo',\n",
      "     'ptruchon',\n",
      "     'purple_circles',\n",
      "     'pyroshroom',\n",
      "     'qman57',\n",
      "     'rafm',\n",
      "     'raquo',\n",
      "     'rbrtwhite',\n",
      "     'rmarescu',\n",
      "     'rmikke',\n",
      "     'robbat2',\n",
      "     'robgeb',\n",
      "     'robidium',\n",
      "     'rudolf',\n",
      "     'rumpelsocke',\n",
      "     'runningcoffee',\n",
      "     'ruthmaben',\n",
      "     'sabbath0802',\n",
      "     'saikofish',\n",
      "     'samely',\n",
      "     'sandil64',\n",
      "     'sansell',\n",
      "     'sarahmprz',\n",
      "     'saynasadeghi',\n",
      "     'sbigelow',\n",
      "     'scotlande',\n",
      "     'sctrojan79',\n",
      "     'seattlefyi',\n",
      "     'skeskali',\n",
      "     'skl1',\n",
      "     'smendoza',\n",
      "     'sotypicat',\n",
      "     'srividya_c',\n",
      "     'staceyfriesen',\n",
      "     'still-a-worm',\n",
      "     'sushiboy',\n",
      "     't-i',\n",
      "     'teraflare',\n",
      "     'the506',\n",
      "     'tippobot',\n",
      "     'tixuwuoz',\n",
      "     'tnightingale',\n",
      "     'torpesco',\n",
      "     'tpaz1',\n",
      "     'tranalex',\n",
      "     'treeniti2',\n",
      "     'tunapeter',\n",
      "     'tynielson',\n",
      "     'uboot',\n",
      "     'user_5359',\n",
      "     'van Rees',\n",
      "     'vanbhills',\n",
      "     'vcelier',\n",
      "     'volz',\n",
      "     'werner2101',\n",
      "     'wheelmap_android',\n",
      "     'wheelmap_visitor',\n",
      "     'willsiddall',\n",
      "     'woodpeck_repair',\n",
      "     'wooster',\n",
      "     'xixi',\n",
      "     'xybot',\n",
      "     'yining wang',\n",
      "     'ypid',\n",
      "     'yukiko',\n",
      "     'z-dude',\n",
      "     'zeitgeist mentor',\n",
      "     'zephyr',\n",
      "     'zeromap',\n",
      "     'zhangrong5301'])\n"
     ]
    }
   ],
   "source": [
    "def get_user(element):\n",
    "    return\n",
    "\n",
    "\n",
    "def process_map(filename):\n",
    "    users = set()\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        pass\n",
    "        if element.tag == \"node\":\n",
    "            user = element.get(\"user\") \n",
    "            if user not in users:\n",
    "                users.add(user) #Note to self: not users.append(user) because users is a set, not a dict\n",
    "\n",
    "    return users\n",
    "\n",
    "pprint.pprint(process_map(osmfile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auditing Street Types\n",
    "[Same as Lesson 6 code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Alley\", \"Broadway\", \"Walk\",\"Crescent\", \"Esplanade\", \"Highway\",\"Kingsway\",\"Mews\",\"Mall\",\"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"South\",\"Commons\",\"Way\",\"West\",\"East\"]\n",
    "\n",
    "# UPDATE THIS VARIABLE\n",
    "mapping = { \"Blvd\": \"Boulevard\",\n",
    "            \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"Ave\": \"Avenue\",\n",
    "            \"Rd.\": \"Road\",\n",
    "            \"Steet\": \"Street\",\n",
    "            \"street\": \"Street\",\n",
    "            \"venue\": \"Avenue\",\n",
    "            \"Broughton\": \"Broughton Street\",\n",
    "            \"Jervis\": \"Jervis Street\",\n",
    "            \"Jarvis\": \"Jervis Street\"\n",
    "            }\n",
    "\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "\n",
    "\n",
    "def update_name(name, mapping):\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    for key, value in mapping.iteritems():\n",
    "        key_at_end = re.escape(key) + r\"$\"\n",
    "        name = re.sub(key_at_end, value, name)\n",
    "\n",
    "    return name\n",
    "\n",
    "st_types = audit(osmfile)\n",
    "pprint.pprint(dict(st_types))\n",
    "for st_type, ways in st_types.iteritems():\n",
    "    for name in ways:\n",
    "        better_name = update_name(name, mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auditing City Names\n",
    "[Adapted from Lesson 6 code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Alfonso': set(['Alfonso']),\n",
      " 'Dresden': set(['Dresden']),\n",
      " 'Edenbridge': set(['Edenbridge']),\n",
      " 'Kirchberg an der Raab': set(['Kirchberg an der Raab']),\n",
      " 'Legnica': set(['Legnica']),\n",
      " 'Lexington': set(['Lexington']),\n",
      " 'Lubatowa': set(['Lubatowa']),\n",
      " 'Nieuw Nickerie': set(['Nieuw Nickerie']),\n",
      " 'North Vancouver': set(['North Vancouver']),\n",
      " 'North Vancouver City': set(['North Vancouver City']),\n",
      " 'Nova Friburgo': set(['Nova Friburgo']),\n",
      " 'Richmond': set(['Richmond']),\n",
      " 'Taguig': set(['Taguig']),\n",
      " 'Vancouver, BC': set(['Vancouver, BC']),\n",
      " 'Vancovuer': set(['Vancovuer']),\n",
      " 'Winsted': set(['Winsted']),\n",
      " 'north vancouver': set(['north vancouver']),\n",
      " 'vancouver': set(['vancouver']),\n",
      " u'\\u0141\\xf3d\\u017a': set([u'\\u0141\\xf3d\\u017a']),\n",
      " u'\\u0415\\u043a\\u0430\\u0442\\u0435\\u0440\\u0438\\u043d\\u0431\\u0443\\u0440\\u0433': set([u'\\u0415\\u043a\\u0430\\u0442\\u0435\\u0440\\u0438\\u043d\\u0431\\u0443\\u0440\\u0433']),\n",
      " u'\\u0422\\u0440\\u0435\\u0442\\u0438\\u0439 \\u0421\\u0435\\u0432\\u0435\\u0440\\u043d\\u044b\\u0439': set([u'\\u0422\\u0440\\u0435\\u0442\\u0438\\u0439 \\u0421\\u0435\\u0432\\u0435\\u0440\\u043d\\u044b\\u0439']),\n",
      " u'\\u8c4a\\u5ddd\\u5e02': set([u'\\u8c4a\\u5ddd\\u5e02'])}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "expected = ['Vancouver', 'North Vancouver', 'Richmond','North Vancouver City']\n",
    "\n",
    "# UPDATE THIS VARIABLE\n",
    "mapping = { \"Vancovuer\": \"Vancouver\",\n",
    "            \"vancouver\": \"Vancouver\",\n",
    "            }\n",
    "\n",
    "\n",
    "def audit_city_name(city_names, city_name):\n",
    "    if city_name not in expected:\n",
    "        city_names[city_name].add(city_name)\n",
    "\n",
    "\n",
    "def is_city_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:city\")\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    city_names = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_city_name(tag):\n",
    "                    audit_city_name(city_names, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return city_names\n",
    "\n",
    "\n",
    "def update_name(name, mapping):\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    for key, value in mapping.iteritems():\n",
    "        key_at_end = re.escape(key) + r\"$\"\n",
    "        name = re.sub(key_at_end, value, name)\n",
    "\n",
    "    return name\n",
    "\n",
    "#'''\n",
    "st_types = audit(osmfile)\n",
    "pprint.pprint(dict(st_types))\n",
    "for st_type, ways in st_types.iteritems():\n",
    "    for name in ways:\n",
    "        better_name = update_name(name, mapping)\n",
    "#'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auditing Province Name\n",
    "[Same as Auditing City Name; I realize that it would be better to have these functions be generic, so that they could handle both city names and province names, but I didn't have time to implement that.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B.C.': set(['B.C.']),\n",
      " 'BC': set(['BC']),\n",
      " 'Cavite': set(['Cavite']),\n",
      " 'Kent': set(['Kent']),\n",
      " 'ON': set(['ON']),\n",
      " 'bc': set(['bc']),\n",
      " u'\\u611b\\u77e5\\u770c': set([u'\\u611b\\u77e5\\u770c'])}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "expected = ['British Columbia']\n",
    "\n",
    "# UPDATE THIS VARIABLE\n",
    "mapping = { \"BC\": \"British Columbia\",\n",
    "            \"B.C.\": \"British Columbia\",\n",
    "            \"bc\": \"British Columbia\",\n",
    "            }\n",
    "\n",
    "def audit_prov_name(prov_names, prov_name):\n",
    "    #m = var_type_re.search(var_name)\n",
    "    if prov_name not in expected:\n",
    "        prov_names[prov_name].add(prov_name)\n",
    "\n",
    "\n",
    "def is_prov_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:province\")\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    prov_names = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_prov_name(tag):\n",
    "                    audit_prov_name(prov_names, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return prov_names\n",
    "\n",
    "\n",
    "def update_name(name, mapping):\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    for key, value in mapping.iteritems():\n",
    "        key_at_end = re.escape(key) + r\"$\"\n",
    "        name = re.sub(key_at_end, value, name)\n",
    "\n",
    "    return name\n",
    "\n",
    "#'''\n",
    "st_types = audit(osmfile)\n",
    "pprint.pprint(dict(st_types))\n",
    "for st_type, ways in st_types.iteritems():\n",
    "    for name in ways:\n",
    "        better_name = update_name(name, mapping)\n",
    "#'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auditing Postal Codes\n",
    "[Adapted from Auditing City Name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'37-114': set(['37-114']),\n",
      " '38-440': set(['38-440']),\n",
      " '4123': set(['4123']),\n",
      " '59-220': set(['59-220']),\n",
      " '620049': set(['620049']),\n",
      " '8324': set(['8324']),\n",
      " '8564': set(['8564']),\n",
      " 'BC V6B': set(['BC V6B']),\n",
      " 'SW11': set(['SW11']),\n",
      " 'TN8 7AH': set(['TN8 7AH']),\n",
      " 'V2J 3A9': set(['V2J 3A9']),\n",
      " 'V3J 3Z9': set(['V3J 3Z9']),\n",
      " 'V5K3A6': set(['V5K3A6']),\n",
      " 'V5K3K3': set(['V5K3K3']),\n",
      " 'V5K5C5': set(['V5K5C5']),\n",
      " 'V5L1K5': set(['V5L1K5']),\n",
      " 'V5L1V3': set(['V5L1V3']),\n",
      " 'V5L2K8': set(['V5L2K8']),\n",
      " 'V5M3K5': set(['V5M3K5']),\n",
      " 'V5N2H7': set(['V5N2H7']),\n",
      " 'V5N2S9': set(['V5N2S9']),\n",
      " 'V5N3P9': set(['V5N3P9']),\n",
      " 'V5R5M2': set(['V5R5M2']),\n",
      " 'V5T1R6': set(['V5T1R6']),\n",
      " 'V5T2K4': set(['V5T2K4']),\n",
      " 'V5T4T6': set(['V5T4T6']),\n",
      " 'V5V3R5': set(['V5V3R5']),\n",
      " 'V5W 0A': set(['V5W 0A']),\n",
      " 'V5X3L6': set(['V5X3L6']),\n",
      " 'V5X4R6': set(['V5X4R6']),\n",
      " 'V5Y4B1': set(['V5Y4B1']),\n",
      " 'V5Z': set(['V5Z']),\n",
      " 'V5Z0A2': set(['V5Z0A2']),\n",
      " 'V5Z1M9': set(['V5Z1M9']),\n",
      " 'V5Z1N2': set(['V5Z1N2']),\n",
      " 'V5Z1N3': set(['V5Z1N3']),\n",
      " 'V5Z2L5': set(['V5Z2L5']),\n",
      " 'V5Z2L7': set(['V5Z2L7']),\n",
      " 'V5Z2R3': set(['V5Z2R3']),\n",
      " 'V5Z2S1': set(['V5Z2S1']),\n",
      " 'V5Z2S4': set(['V5Z2S4']),\n",
      " 'V5Z3M5': set(['V5Z3M5']),\n",
      " 'V5Z3M9': set(['V5Z3M9']),\n",
      " 'V5Z3N2': set(['V5Z3N2']),\n",
      " 'V5Z3N5': set(['V5Z3N5']),\n",
      " 'V5Z3N6': set(['V5Z3N6']),\n",
      " 'V5Z3S2': set(['V5Z3S2']),\n",
      " 'V6': set(['V6']),\n",
      " 'V6B0G6': set(['V6B0G6']),\n",
      " 'V6B1G6': set(['V6B1G6']),\n",
      " 'V6B1H7': set(['V6B1H7']),\n",
      " 'V6B2J1': set(['V6B2J1']),\n",
      " 'V6B2N4': set(['V6B2N4']),\n",
      " 'V6C': set(['V6C']),\n",
      " 'V6C OC3': set(['V6C OC3']),\n",
      " 'V6E1N4': set(['V6E1N4']),\n",
      " 'V6E2P4': set(['V6E2P4']),\n",
      " 'V6G1X5': set(['V6G1X5']),\n",
      " 'V6G1Y7': set(['V6G1Y7']),\n",
      " 'V6G2W3': set(['V6G2W3']),\n",
      " 'V6G3E3': set(['V6G3E3']),\n",
      " 'V6H1L6': set(['V6H1L6']),\n",
      " 'V6H2K5': set(['V6H2K5']),\n",
      " 'V6H2R2': set(['V6H2R2']),\n",
      " 'V6H2R5': set(['V6H2R5']),\n",
      " 'V6H3J7': set(['V6H3J7']),\n",
      " 'V6J0E2': set(['V6J0E2']),\n",
      " 'V6J1M3': set(['V6J1M3']),\n",
      " 'V6J3H1': set(['V6J3H1']),\n",
      " 'V6J5G4': set(['V6J5G4']),\n",
      " 'V6K4T4': set(['V6K4T4']),\n",
      " 'V6M1L8': set(['V6M1L8']),\n",
      " 'V6M1L9': set(['V6M1L9']),\n",
      " 'V6M1M1': set(['V6M1M1']),\n",
      " 'V6M1M2': set(['V6M1M2']),\n",
      " 'V6M1M3': set(['V6M1M3']),\n",
      " 'V6M1P6': set(['V6M1P6']),\n",
      " 'V6M1P8': set(['V6M1P8']),\n",
      " 'V6M1P9': set(['V6M1P9']),\n",
      " 'V6M1R4': set(['V6M1R4']),\n",
      " 'V6M1S7': set(['V6M1S7']),\n",
      " 'V6M1S9': set(['V6M1S9']),\n",
      " 'V6M1T1': set(['V6M1T1']),\n",
      " 'V6M1V1': set(['V6M1V1']),\n",
      " 'V6M1V2': set(['V6M1V2']),\n",
      " 'V6M1V3': set(['V6M1V3']),\n",
      " 'V6M1V5': set(['V6M1V5']),\n",
      " 'V6M1W8': set(['V6M1W8']),\n",
      " 'V6M1X1': set(['V6M1X1']),\n",
      " 'V6M1X2': set(['V6M1X2']),\n",
      " 'V6M1X4': set(['V6M1X4']),\n",
      " 'V6M1X5': set(['V6M1X5']),\n",
      " 'V6M2A8': set(['V6M2A8']),\n",
      " 'V6M2B8': set(['V6M2B8']),\n",
      " 'V6M2G8': set(['V6M2G8']),\n",
      " 'V6M2H1': set(['V6M2H1']),\n",
      " 'V6M2P7': set(['V6M2P7']),\n",
      " 'V6M2V5': set(['V6M2V5']),\n",
      " 'V6M2W7': set(['V6M2W7']),\n",
      " 'V6M2W9': set(['V6M2W9']),\n",
      " 'V6M2X2': set(['V6M2X2']),\n",
      " 'V6M2X4': set(['V6M2X4']),\n",
      " 'V6M2Y5': set(['V6M2Y5']),\n",
      " 'V6M2Z1': set(['V6M2Z1']),\n",
      " 'V6M2Z2': set(['V6M2Z2']),\n",
      " 'V6M2Z4': set(['V6M2Z4']),\n",
      " 'V6M3A3': set(['V6M3A3']),\n",
      " 'V6M3A5': set(['V6M3A5']),\n",
      " 'V6M3A6': set(['V6M3A6']),\n",
      " 'V6M4J6': set(['V6M4J6']),\n",
      " 'V6P3M4': set(['V6P3M4']),\n",
      " 'V6R3S5': set(['V6R3S5']),\n",
      " 'V6T1L9': set(['V6T1L9']),\n",
      " 'V6T1Z1': set(['V6T1Z1']),\n",
      " 'V6T1Z3': set(['V6T1Z3']),\n",
      " 'V6Z 2v9': set(['V6Z 2v9']),\n",
      " 'V6Z1M1': set(['V6Z1M1']),\n",
      " 'V6Z1M4': set(['V6Z1M4']),\n",
      " 'V6Z1R2': set(['V6Z1R2']),\n",
      " 'V6Z2T9': set(['V6Z2T9']),\n",
      " 'V6hH 2R5': set(['V6hH 2R5']),\n",
      " 'V7J 1A4': set(['V7J 1A4']),\n",
      " 'V7J 1C4': set(['V7J 1C4']),\n",
      " 'V7J 1C9': set(['V7J 1C9']),\n",
      " 'V7J 2C1': set(['V7J 2C1']),\n",
      " 'V7J 2K7': set(['V7J 2K7']),\n",
      " 'V7L0B2': set(['V7L0B2']),\n",
      " 'V7L2A4': set(['V7L2A4']),\n",
      " 'V7P 3S1': set(['V7P 3S1']),\n",
      " 'V7Y 1A1': set(['V7Y 1A1']),\n",
      " 'V7Y 1K8': set(['V7Y 1K8']),\n",
      " 'V7Y1E8': set(['V7Y1E8']),\n",
      " 'v5l4t6': set(['v5l4t6']),\n",
      " 'v5t 4r8': set(['v5t 4r8']),\n",
      " 'v5y 3w3': set(['v5y 3w3']),\n",
      " 'v6z 2b7': set(['v6z 2b7']),\n",
      " 'v7m 2e8': set(['v7m 2e8'])}\n"
     ]
    }
   ],
   "source": [
    "expected_postalcodes_re = re.compile(r'V[5|6][A-Z] [0-9][A-Z][0-9]$')\n",
    "\n",
    "# UPDATE THIS VARIABLE\n",
    "\n",
    "def audit_postal_code(postal_codes, postal_code):\n",
    "    if not expected_postalcodes_re.search(postal_code):\n",
    "        postal_codes[postal_code].add(postal_code)\n",
    "\n",
    "\n",
    "def is_postal_code(elem):\n",
    "    return (elem.attrib['k'] == \"addr:postcode\")\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    postal_codes = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_postal_code(tag):\n",
    "                    audit_postal_code(postal_codes, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return postal_codes\n",
    "\n",
    "\n",
    "def update_name(name, mapping):\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    for key, value in mapping.iteritems():\n",
    "        key_at_end = re.escape(key) + r\"$\"\n",
    "        name = re.sub(key_at_end, value, name)\n",
    "\n",
    "    return name\n",
    "\n",
    "#'''\n",
    "st_types = audit(osmfile)\n",
    "pprint.pprint(dict(st_types))\n",
    "for st_type, ways in st_types.iteritems():\n",
    "    for name in ways:\n",
    "        better_name = update_name(name, mapping)\n",
    "#'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for printing out file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shape_element(element):\n",
    "    print element\n",
    "    if element.tag == \"node\" or element.tag == \"way\" or element.tag == \"relation\" :\n",
    "        for key, value in element.attrib.iteritems():\n",
    "            print 'key', 'value', key, value\n",
    "        \n",
    "\n",
    "        for child in element:\n",
    "            print 'child', child\n",
    "            for key, value in child.attrib.iteritems():\n",
    "                print \"key, value\", key, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for preparing for MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "CREATED = [ \"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "\n",
    "\n",
    "def shape_element(element):\n",
    "    node = {}\n",
    "    if element.tag == \"node\" or element.tag == \"way\" :\n",
    "        # YOUR CODE HERE\n",
    "        this_created = {}\n",
    "        node['type'] = element.tag\n",
    "        # based on https://discussions.udacity.com/t/my-code-is-not-finding-a-specific-tag-attribute/157674/5\n",
    "        \n",
    "        for key, value in element.attrib.iteritems():\n",
    "            #attributes in the CREATED array should be added under a key \"created\"\n",
    "            #attributes for latitude and longitude should be added to a \"pos\" array,for use in geospacial indexing. Make sure the values inside \"pos\" array are floats and not strings. \n",
    "            created_list = ['version','changeset','timestamp','user','uid']\n",
    "            if key in created_list:\n",
    "                this_created[key] = element.attrib[key]\n",
    "            \n",
    "            elif key == 'lon':\n",
    "                pass      # will be dealt with in 'lat' \n",
    "            elif key == 'lat': \n",
    "                node['pos'] = [float(element.attrib['lat']), float(element.attrib['lon'])]          \n",
    "            else:\n",
    "                node[key] = value\n",
    "        node['created'] = this_created\n",
    "        if element.tag == \"way\":\n",
    "            node['node_refs'] = []\n",
    "                \n",
    "            #if the second level tag \"k\" value starts with \"addr:\", it should be added to a dictionary \"address\"\n",
    "        this_address = {}\n",
    "        keyfornode = \"\"\n",
    "        for child in element: #note: not an interior loop of above, though \n",
    "            for key, value in child.attrib.iteritems():\n",
    "                #if the second level tag \"k\" value contains problematic characters, it should be ignored\n",
    "                if 'k' in child.attrib: #make sure that there is a key tag\n",
    "                    if  re.search(problemchars, child.attrib['k']):     \n",
    "                        #pass\n",
    "                        print \"k Has problem chars: \", child.attrib['k']\n",
    "                    elif re.match('addr:',child.attrib['k']): #if k tag starts with addr:\n",
    "                        if child.attrib['k'].count(':') > 1: #if k tag has 2 colons, ignore\n",
    "                            pass\n",
    "                        else:\n",
    "                            temp = child.attrib['k'].split(':')[1]\n",
    "                            this_address[temp] = child.attrib['v']\n",
    "                            #print this_address\n",
    "                    \n",
    "                    elif child.attrib['k'].count(':') > 0: #if k tag does not start with addr: but has a colon\n",
    "                        #pass\n",
    "                        \n",
    "                        #print 'k Has colon: ', child.attrib['k']\n",
    "                        if child.attrib['k'].count(':') > 1: #if k tag has 2 colons, ignore\n",
    "                            pass\n",
    "                        else:\n",
    "                            # capture the content before the colon\n",
    "                            if keyfornode == \"\" or keyfornode != child.attrib['k'].split(':')[0]:\n",
    "                                keyfornode = child.attrib['k'].split(':')[0]\n",
    "                                this_other = {}\n",
    "                            subkey = child.attrib['k'].split(':')[1]\n",
    "                            this_other[subkey] = child.attrib['v']\n",
    "                            node[keyfornode] = this_other #update\n",
    "                            #print this_address\n",
    "                    else:\n",
    "                        #pass\n",
    "                        #print 'k is something else: ', child.attrib['k']\n",
    "                        node[child.attrib['k']] = child.attrib['v']\n",
    "                elif 'ref' in child.attrib:\n",
    "                    node['node_refs'].append(child.attrib['ref'])\n",
    "                    #print node['node_refs']\n",
    "                elif 'v' not in child.attrib:\n",
    "                     print 'the child.attrib is something other than k, v, or ref: ', child.attrib\n",
    "                   \n",
    "                else:\n",
    "                    pass\n",
    "        if this_address: #if this_address is not empty\n",
    "            node['address'] = this_address\n",
    "\n",
    "        \n",
    "        #print node\n",
    "        return node\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Code for dumping to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_map(file_in, pretty = False):\n",
    "    # You do not need to change this file\n",
    "    file_out = \"{0}.json\".format(file_in)\n",
    "    data = []\n",
    "    with codecs.open(file_out, \"w\") as fo:\n",
    "        for _, element in ET.iterparse(file_in):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                data.append(el)\n",
    "                if pretty:\n",
    "                    fo.write(json.dumps(el, indent=2)+\"\\n\")\n",
    "                else:\n",
    "                    fo.write(json.dumps(el) + \"\\n\")\n",
    "    return data\n",
    "data = process_map(osmfile, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
